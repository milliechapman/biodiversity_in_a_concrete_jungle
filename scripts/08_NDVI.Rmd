---
title: "whats up with wetlands?"
author: "Dexter H. Locke, PhD"
date: "`r format(Sys.time())`"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


# 0 set up: load libraries, custom functions, set defaults
```{r}

# load libraries
# packages we'll be using
packs <- c(
    'tidyverse'  # a must have
  , 'tidylog'    # prints out what was done in dplyr and tidr
  # , 'gbifdb' # GBIF
  # , 'fst' # a faster table, makes outputs files much smaller, too.
  # , 'terra'
  # , 'KnowBR'    # creates biodiversity estimates like completeness.
  # , 'tidycensus'      # Census access
  , 'sf'        # spatial support
  , 'mapview'   # webmaps
  , 'janitor'   # cleans things up, also pipe-friendly cross-tabulations
  , 'tictoc'    # times things
  , 'beepr'     # makes noises
)


# check for all of the libraries, install if you don't have them
if (length(setdiff(packs, rownames(installed.packages()))) > 0) {
  install.packages(setdiff(packs, rownames(installed.packages())))  
}

# load them
vapply(packs, library, character.only = TRUE, logical(1), logical.return = TRUE, quietly = TRUE)



# custom function for "Not In"
`%nin%` <- Negate(`%in%`)


# redlining colors
holc_pal <- c('#92BC6B' # green
              , '#92C7C9' # blue
              , '#E7DC6B' # yellow
              , '#E47D67' # red
              #, '#A9A9A9'
              ) # dark gray)

holc_pal_f<- c('#92BC6B' # green
              , '#92C7C9' # blue
              , '#E7DC6B' # yellow
              , '#E47D67' # red
              , '#A9A9A9'
              , '#000000')

# fixes mapview
mapviewOptions(fgb = FALSE)


# Paralise any simple features analysis.
# https://www.spatialanalytics.co.nz/post/2018/04/01/fixing-st-par/
# define
st_parallel <- function(sf_df, sf_func, n_cores, ...){

  # Create a vector to split the data set up by.
  split_vector <- rep(1:n_cores, each = nrow(sf_df) / n_cores, length.out = nrow(sf_df))

  # Perform GIS analysis
  split_results <- split(sf_df, split_vector) %>%
    parallel::mclapply(function(x) sf_func(x, ...), mc.cores = n_cores)
  
  
  # Define the output_class. If length is greater than two, then grab the second variable.
  output_class <- class(split_results[[1]])
  if (length(output_class) == 2){
    output_class <- output_class[2]
  }
  
  # Combine results back together. Method of combining depends on the output from the function.
  if (output_class == "matrix"){
    result <- do.call("rbind", split_results)
    names(result) <- NULL
  } else if (output_class == "sfc") {
    result <- do.call("c", split_results)
    result <- sf_func(result) # do.call combines the list but there are still n_cores of the geometry which had been split up. Running st_union or st_collect gathers them up into one, as is the expected output of these two functions. 
  } else if (output_class %in% c('list', 'sgbp') ){
    result <- do.call("c", split_results)
    names(result) <- NULL
  } else if (output_class == "data.frame" ){
    result <- do.call("rbind", split_results)
  } else {
    stop("Unknown class. st_parallel only accepts the following outputs at present: sfc, list, sf, matrix, sgbp.")
  }
  
  # Return result
  return(result)
}
```

# 1 compile all NDVI files
```{r}

list.files('working_data/ndvi')

ndvi <- 
  tibble(
      filename = list.files('working_data/ndvi', full.names = TRUE) |>
        str_subset('polys_missing_ndvi', negate = TRUE) # drops results of a test/dobule check
    , file_contents = map(filename, ~read_csv(., col_types = cols())) # makes less-verbose
    ) |> 
  mutate(unit = str_remove_all(filename, 'working_data/ndvi/modis250_ndvi_') |> 
           str_remove_all('_2018.csv') |> 
           str_remove_all('_2019.csv') |> 
           str_remove_all('_2020.csv') |> 
           str_remove_all('_2021.csv') |> 
           str_remove_all('_2022.csv')
           ) |> 
  unnest(file_contents) |> 
  select(unit, id, msa_GEOID = m_GEOID, ua_GEOID = GEOID, mean, max) # reduce and rename cols

ndvi # totally rules!

  
```


# 2 aggregate by source
## A HOLC
```{r}

(holc <- st_read('working_data/holc_polys_saved/holc_plys_2022-12-08 20-13-17.gpkg') |> 
   st_collection_extract() |> 
   st_make_valid())

ndvi |> arrange(id)

holc_ndvi_summs <- 
  ndvi |> 
  # mutate(id = str_squish(id)) |> 
  group_by(id) |>
  summarise(mean_ndvi = mean(mean))

# there are some NA's that can be rescued
fold_ins <- 
  tribble(
    ~'id', ~'mean_NDVI', 
    # some areas had NAs, Deigo re-ran in Google Earth Engine and rescued a few polygons
    # "NY_Manhattan_A1_A_6419",null]
    # "NY_Manhattan_A2_A_6420",null]
    # "NY_Manhattan_B1_B_6427",null]
    # "NY_Manhattan_B2_B_6431",null]
      "NY_Manhattan_B3_B_6432", 0.4551142857142857
    , "NY_Manhattan_C1_C_6439", -0.03462424050632912
    # "NY_Manhattan_C2_C_6440",null]
    , "NY_Queens_C101_C_6536", 0.05513408311001995
    , "NY_Manhattan_D1_D_6445", -0.012410000000000001
    , "NY_Manhattan_D10_D_6446", -0.021987613843351548
    # "NY_Manhattan_D11_D_6447",null]
    # "NY_Manhattan_D12_D_6448",null]
    , "NY_Manhattan_D13_D_6449", 0.1839243206754305
    , "NY_Manhattan_D2_D_6456", 0.12490000000000001
    , "NY_Manhattan_D4_D_6466", -0.07473333333333335
    , "NY_Manhattan_D5_D_6467", -0.015716814536340853
    , "NY_Manhattan_D6_D_6468", 0.18939010025062658
    # "NY_Manhattan_D8_D_6470",null]
    # "NY_Manhattan_D9_D_6471",null]
    )

fold_ins

# What about the NAs?
holc_ndvi_summs |> 
  filter(id %in% fold_ins$id) 

# can 'rescue' some though
ndvi |> 
  left_join(fold_ins, by = 'id') |> 
  filter(!is.na(mean_NDVI))

# update 50 rows, yaay!
ndvi <-
  ndvi |> 
  left_join(fold_ins, by = 'id') |> 
  mutate(mean = ifelse(!is.na(mean_NDVI), mean_NDVI, mean)) ## |> filter(!is.na(mean_NDVI))
    
# double check
holc |> anti_join(holc_ndvi_summs, by = 'id') # fails perfect!

holc |> 
  left_join(holc_ndvi_summs, by = 'id')

# still some NA's but we don't have data..
holc_ndvi_summs |> summary()

# write out
holc_ndvi_summs |> 
  write_csv('working_data/ndvi_holc.csv')


# # what does NOT join?
# (not_joined <- holc |> anti_join(ndvi_summs, by = 'id'))
# 
# not_joined |>
#   st_collection_extract() |> 
#   sample_n(1) |> 
#   mapview()
# 
# holc |> 
#   filter(id %in% not_joined$id) |> 
#   st_collection_extract() |> 
#   sample_n(1) |> 
#   mapview()
# 
# holc |> 
#   mutate(id = str_squish(id)) |> 
#   left_join(not_joined |> 
#               st_drop_geometry() |> 
#               mutate(id = str_squish(id))
#             , by = 'id'
#             )

```


## B MSA and ungraded MSAs
```{r}

msa  <- st_read('working_data/MSA/msa_as_geopackage_2022-12-08 15-40-07.gpkg')
msau <- st_read('working_data/MSA_donut/msa_ungraded.gpkg')

# do they match
msa |> anti_join(msau |> st_drop_geometry()) # yes!

# checks
ndvi |> group_by(msa_GEOID, unit) |> count()
ndvi |> group_by(msa_GEOID, unit) |> count() |> summary()
ndvi |> group_by(msa_GEOID, unit) |> count() |> tail()

(
  msa_ndvi_summ <- 
    ndvi |> 
    filter(unit == 'msa' | unit == 'msau') |> 
    group_by(msa_GEOID, unit) |> 
    summarise(mean_ndvi = mean(mean, na.rm = TRUE)) |> 
    mutate(msa_GEOID = as.character(msa_GEOID)) |> 
    select(-msa_GEOID)
  )

# check the joins

# MSA
msa |> 
  left_join(
    msa_ndvi_summ |> filter(unit == 'msa')
    , by = 'msa_GEOID'
    )

# MSA erased
msau |> 
  left_join(
    msa_ndvi_summ |> filter(unit == 'msau')
    , by = 'msa_GEOID'
    )

# write out
msa_ndvi_summ |> 
  write_csv('working_data/ndvi_msa_msau.csv')

```

## C UA and ungraded UAs
```{r}

ua  <- st_read('working_data/UA/UA_as_geopackage_2023-01-23 15-48-08.gpkg')
uau  <- st_read('working_data/UA_donut/UA_ungraded_2023-01-23 16-05-42.gpkg')

# do they match
ua |> anti_join(uau |> st_drop_geometry()) # yes!

# checks
ndvi |> group_by(ua_GEOID, unit) |> count()
ndvi |> group_by(ua_GEOID, unit) |> count() |> summary()
ndvi |> group_by(ua_GEOID, unit) |> count() |> tail()

(
  ua_ndvi_summ <- 
    ndvi |> 
    filter(unit == 'ua' | unit == 'uau') |> 
    group_by(ua_GEOID, unit) |> 
    summarise(mean_ndvi = mean(mean, na.rm = TRUE))
  )

# check the joins

# UA
ua |> 
  left_join(
    ua_ndvi_summ |> filter(unit == 'ua')
    , by = c('GEOID' = 'ua_GEOID')
    )

# UA erased
uau |> 
  left_join(
    ua_ndvi_summ |> filter(unit == 'uau')
    , by = c('GEOID' = 'ua_GEOID')
    )

# write out
ua_ndvi_summ |> 
  write_csv('working_data/ndvi_ua_uau.csv')

```


msa_u <- st_read('working_data/MSA_donut/msa_ungraded.gpkg')

```{r}

# read through each of the geography-specific files, group_by. and create mean.


# 
# 
# # list.files('working_data/MSA')
# # msa   <- st_read('working_data/MSA/msa_as_geopackage_2022-12-08 15-40-07.gpkg')
# # msa_u <- st_read('working_data/MSA_donut/msa_ungraded.gpkg')
# 
# list.files('working_data/ndvi')
# list.files('working_data')
# 
# ndvi <- read_csv('working_data/mean_ndvi.csv')
# 
# # MSA: we tried to make ndvi character, then we tried make msa numeric.
# msa |> 
#   mutate(GEOID = as.numeric(msa_GEOID)) |> 
#   left_join(
#     ndvi |>
#       filter(area == 'msa') #|> 
#       # mutate(msa_GEOID = as.character(GEOID))
#       , by = c('GEOID')
#     ) |> 
#   filter(!is.na(mean_ndvi)) # none matched
# 
# 
# # M
# msa |> 
#   left_join(
#     ndvi |>
#       filter(area == 'msa') |> 
#       mutate(msa_GEOID = as.character(GEOID))
#       , by = c('msa_GEOID'))
# 
# 
# 
# # test_ndvi <- read_csv("working_data/ndvi/modis250_ndvi_msa_2018.csv")
# test_ndvi <- read_csv("working_data/ndvi/modis250_ndvi_")
# 
# msa |> 
#   mutate(msa_GEOID = as.double(msa_GEOID)) |> 
#   left_join(test_ndvi, by = c('msa_GEOID' = 'm_GEOID'))



```




