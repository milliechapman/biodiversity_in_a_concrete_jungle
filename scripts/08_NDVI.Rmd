---
title: "whats up with wetlands?"
author: "Dexter H. Locke, PhD"
date: "`r format(Sys.time())`"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


# 0 set up: load libraries, custom functions, set defaults
```{r}

# load libraries
# packages we'll be using
packs <- c(
    'tidyverse'  # a must have
  , 'tidylog'    # prints out what was done in dplyr and tidr
  # , 'gbifdb' # GBIF
  # , 'fst' # a faster table, makes outputs files much smaller, too.
  # , 'terra'
  # , 'KnowBR'    # creates biodiversity estimates like completeness.
  # , 'tidycensus'      # Census access
  , 'sf'        # spatial support
  , 'mapview'   # webmaps
  , 'janitor'   # cleans things up, also pipe-friendly cross-tabulations
  , 'tictoc'    # times things
  , 'beepr'     # makes noises
)


# check for all of the libraries, install if you don't have them
if (length(setdiff(packs, rownames(installed.packages()))) > 0) {
  install.packages(setdiff(packs, rownames(installed.packages())))  
}

# load them
vapply(packs, library, character.only = TRUE, logical(1), logical.return = TRUE, quietly = TRUE)



# custom function for "Not In"
`%nin%` <- Negate(`%in%`)


# redlining colors
holc_pal <- c('#92BC6B' # green
              , '#92C7C9' # blue
              , '#E7DC6B' # yellow
              , '#E47D67' # red
              #, '#A9A9A9'
              ) # dark gray)

holc_pal_f<- c('#92BC6B' # green
              , '#92C7C9' # blue
              , '#E7DC6B' # yellow
              , '#E47D67' # red
              , '#A9A9A9'
              , '#000000')

# fixes mapview
mapviewOptions(fgb = FALSE)


# Paralise any simple features analysis.
# https://www.spatialanalytics.co.nz/post/2018/04/01/fixing-st-par/
# define
st_parallel <- function(sf_df, sf_func, n_cores, ...){

  # Create a vector to split the data set up by.
  split_vector <- rep(1:n_cores, each = nrow(sf_df) / n_cores, length.out = nrow(sf_df))

  # Perform GIS analysis
  split_results <- split(sf_df, split_vector) %>%
    parallel::mclapply(function(x) sf_func(x, ...), mc.cores = n_cores)
  
  
  # Define the output_class. If length is greater than two, then grab the second variable.
  output_class <- class(split_results[[1]])
  if (length(output_class) == 2){
    output_class <- output_class[2]
  }
  
  # Combine results back together. Method of combining depends on the output from the function.
  if (output_class == "matrix"){
    result <- do.call("rbind", split_results)
    names(result) <- NULL
  } else if (output_class == "sfc") {
    result <- do.call("c", split_results)
    result <- sf_func(result) # do.call combines the list but there are still n_cores of the geometry which had been split up. Running st_union or st_collect gathers them up into one, as is the expected output of these two functions. 
  } else if (output_class %in% c('list', 'sgbp') ){
    result <- do.call("c", split_results)
    names(result) <- NULL
  } else if (output_class == "data.frame" ){
    result <- do.call("rbind", split_results)
  } else {
    stop("Unknown class. st_parallel only accepts the following outputs at present: sfc, list, sf, matrix, sgbp.")
  }
  
  # Return result
  return(result)
}
```

# 1 compile all NDVI files
```{r}

list.files('working_data/ndvi')


ndvi <- 
  tibble(
    filename = list.files('working_data/ndvi', full.names = TRUE)
    , file_contents = map(filename, ~read_csv(., col_types = cols())) # surpresses warnings
    ) |> 
  mutate(unit = str_remove_all(filename, 'working_data/ndvi/modis250_ndvi_') |> 
           str_remove_all('_2018.csv') |> 
           str_remove_all('_2019.csv') |> 
           str_remove_all('_2020.csv') |> 
           str_remove_all('_2021.csv') |> 
           str_remove_all('_2022.csv')
           ) |> 
  unnest(file_contents) |> 
  select(unit, id, m_GEOID, GEOID, mean, max)

ndvi # kinda rad
```


# 2 aggregate by source
## A HOLC
```{r}

(holc <- st_read('working_data/holc_polys_saved/holc_plys_2022-12-08 20-13-17.gpkg'))

ndvi |> arrange(id)

test <- 
  ndvi |> 
  # mutate(id = str_squish(id)) |> 
  group_by(id) |>
  summarise(mean_ndvi = mean(mean))

holc |> anti_join(test)

holc |> 
  mutate(id = str_squish(id)) |> 
  left_join(test, by = 'id') |> filter(is.na(mean_ndvi))

```


## B MSA and ungraded MSAs
```{r}

msa  <- st_read('working_data/MSA/msa_as_geopackage_2022-12-08 15-40-07.gpkg')
msau <- st_read('working_data/MSA_donut/msa_ungraded.gpkg')

# do they match
msa |> anti_join(msau |> st_drop_geometry()) # yes!

# checks
ndvi |> group_by(m_GEOID, unit) |> count()
ndvi |> group_by(m_GEOID, unit) |> count() |> summary()
ndvi |> group_by(m_GEOID, unit) |> count() |> tail()

(
  msa_ndvi_summ <- 
    ndvi |> 
    filter(unit == 'msa' | unit == 'msau') |> 
    group_by(m_GEOID, unit) |> 
    summarise(mean_ndvi = mean(mean, na.rm = TRUE)) |> 
    mutate(msa_GEOID = as.character(m_GEOID)) |> 
    select(-m_GEOID)
  )

# check the joins

# MSA
msa |> 
  left_join(
    msa_ndvi_summ |> filter(unit == 'msa')
    , by = 'msa_GEOID'
    )

# MSA erased
msau |> 
  left_join(
    msa_ndvi_summ |> filter(unit == 'msau')
    , by = 'msa_GEOID'
    )

# write out
msa_ndvi_summ |> 
  write_csv('working_data/ndvi_msa_msau.csv')

```

## C UA and ungraded UAs
```{r}

ua  <- st_read('working_data/UA/UA_as_geopackage_2023-01-23 15-48-08.gpkg')
uau  <- st_read('working_data/UA_donut/UA_ungraded_2023-01-23 16-05-42.gpkg')

# do they match
ua |> anti_join(uau |> st_drop_geometry()) # yes!

# checks
ndvi |> group_by(GEOID, unit) |> count()
ndvi |> group_by(GEOID, unit) |> count() |> summary()
ndvi |> group_by(GEOID, unit) |> count() |> tail()

(
  ua_ndvi_summ <- 
    ndvi |> 
    filter(unit == 'ua' | unit == 'uau') |> 
    group_by(GEOID, unit) |> 
    summarise(mean_ndvi = mean(mean, na.rm = TRUE))
  )

# check the joins

# UA
ua |> 
  left_join(
    ua_ndvi_summ |> filter(unit == 'ua')
    , by = 'GEOID'
    )

# UA erased
uau |> 
  left_join(
    ua_ndvi_summ |> filter(unit == 'uau')
    , by = 'GEOID'
    )

# write out
ua_ndvi_summ |> 
  write_csv('working_data/ndvi_ua_uau.csv')

```


msa_u <- st_read('working_data/MSA_donut/msa_ungraded.gpkg')

```{r}

# read through each of the geography-specific files, group_by. and create mean.


# 
# 
# # list.files('working_data/MSA')
# # msa   <- st_read('working_data/MSA/msa_as_geopackage_2022-12-08 15-40-07.gpkg')
# # msa_u <- st_read('working_data/MSA_donut/msa_ungraded.gpkg')
# 
# list.files('working_data/ndvi')
# list.files('working_data')
# 
# ndvi <- read_csv('working_data/mean_ndvi.csv')
# 
# # MSA: we tried to make ndvi character, then we tried make msa numeric.
# msa |> 
#   mutate(GEOID = as.numeric(msa_GEOID)) |> 
#   left_join(
#     ndvi |>
#       filter(area == 'msa') #|> 
#       # mutate(msa_GEOID = as.character(GEOID))
#       , by = c('GEOID')
#     ) |> 
#   filter(!is.na(mean_ndvi)) # none matched
# 
# 
# # M
# msa |> 
#   left_join(
#     ndvi |>
#       filter(area == 'msa') |> 
#       mutate(msa_GEOID = as.character(GEOID))
#       , by = c('msa_GEOID'))
# 
# 
# 
# # test_ndvi <- read_csv("working_data/ndvi/modis250_ndvi_msa_2018.csv")
# test_ndvi <- read_csv("working_data/ndvi/modis250_ndvi_")
# 
# msa |> 
#   mutate(msa_GEOID = as.double(msa_GEOID)) |> 
#   left_join(test_ndvi, by = c('msa_GEOID' = 'm_GEOID'))



```




