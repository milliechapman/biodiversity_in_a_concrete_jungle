---
title: "whats up with wetlands?"
author: "Dexter H. Locke, PhD"
date: "`r format(Sys.time())`"
output: html_document
editor_options: 
  chunk_output_type: console
---

# rasterize these polygons
#     fasterize
# then summarize the pixels per polygon


https://www.fws.gov/program/national-wetlands-inventory/download-state-wetlands-data

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# 0 set up: load libraries, custom functions, set defaults
```{r}

# load libraries
# packages we'll be using
packs <- c(
    'tidyverse'  # a must have
  , 'tidylog'    # prints out what was done in dplyr and tidr
  # , 'gbifdb' # GBIF
  # , 'fst' # a faster table, makes outputs files much smaller, too.
  # , 'terra'
  # , 'KnowBR'    # creates biodiversity estimates like completeness.
  , 'tidycensus'      # Census access
  , 'sf'        # spatial support
  , 'mapview'   # webmaps
  , 'janitor'   # cleans things up, also pipe-friendly cross-tabulations
  , 'tictoc'    # times things
  , 'beepr'     # makes noises
  , 'sfarrow'
  , 'ggpubr'    # boxplots with significance testing.
)


# check for all of the libraries, install if you don't have them
if (length(setdiff(packs, rownames(installed.packages()))) > 0) {
  install.packages(setdiff(packs, rownames(installed.packages())))  
}

# load them
vapply(packs, library, character.only = TRUE, logical(1), logical.return = TRUE, quietly = TRUE)



# custom function for "Not In"
`%nin%` <- Negate(`%in%`)


# redlining colors
holc_pal <- c('#92BC6B' # green
              , '#92C7C9' # blue
              , '#E7DC6B' # yellow
              , '#E47D67' # red
              #, '#A9A9A9'
              ) # dark gray)

holc_pal_f<- c('#92BC6B' # green
              , '#92C7C9' # blue
              , '#E7DC6B' # yellow
              , '#E47D67' # red
              , '#A9A9A9'
              , '#000000')

# fixes mapview
mapviewOptions(fgb = FALSE)


# Paralise any simple features analysis.
# https://www.spatialanalytics.co.nz/post/2018/04/01/fixing-st-par/
# define
st_parallel <- function(sf_df, sf_func, n_cores, ...){

  # Create a vector to split the data set up by.
  split_vector <- rep(1:n_cores, each = nrow(sf_df) / n_cores, length.out = nrow(sf_df))

  # Perform GIS analysis
  split_results <- split(sf_df, split_vector) %>%
    parallel::mclapply(function(x) sf_func(x, ...), mc.cores = n_cores)
  
  
  # Define the output_class. If length is greater than two, then grab the second variable.
  output_class <- class(split_results[[1]])
  if (length(output_class) == 2){
    output_class <- output_class[2]
  }
  
  # Combine results back together. Method of combining depends on the output from the function.
  if (output_class == "matrix"){
    result <- do.call("rbind", split_results)
    names(result) <- NULL
  } else if (output_class == "sfc") {
    result <- do.call("c", split_results)
    result <- sf_func(result) # do.call combines the list but there are still n_cores of the geometry which had been split up. Running st_union or st_collect gathers them up into one, as is the expected output of these two functions. 
  } else if (output_class %in% c('list', 'sgbp') ){
    result <- do.call("c", split_results)
    names(result) <- NULL
  } else if (output_class == "data.frame" ){
    result <- do.call("rbind", split_results)
  } else {
    stop("Unknown class. st_parallel only accepts the following outputs at present: sfc, list, sf, matrix, sgbp.")
  }
  
  # Return result
  return(result)
}
```

# 1 which states do we need to download?
```{r}

# pull from msa
msa <- st_read('working_data/MSA/msa_as_geopackage_2022-12-08 15-40-07.gpkg')

states_msa <- 
  msa |> 
  st_drop_geometry() |> 
  # tibble() |> 
  select(msa_name) |> 
  mutate(msa_name = str_remove(msa_name,  ' Metro Area') |> 
           str_remove(' Micro Area')) |> 
  separate(msa_name, into = c('trash', 'state'), sep = ', ') |> 
  separate(state, into = c('state1', 'state2', 'state3', 'state4'), sep = '-') |> 
  select(starts_with('state')) |> 
  rowid_to_column() |> 
  pivot_longer(-rowid) |> 
  distinct(value) |> 
  filter(!is.na(value)) |> 
  arrange(value) |> data.frame()# 40

(states <- 
  st_read('working_data/holc_polys_saved/holc_plys_2022-12-08 20-13-17.gpkg') |> 
  st_drop_geometry() |> 
  distinct(state)) # 38

# go with MSA-derived states!
states_msa |> left_join(states, by = c('value' = 'state'), keep = TRUE)

(states <- states_msa); rm(states_msa)
```

# 2 unzip
## A geodatabase - defunct, did not work (erroneous geometries)
```{r eval=FALSE, include=FALSE}

list.files('../biodiversity_in_a_concrete_jungle_data_too_big/wetlands/zipped_wetlands')

# # get zip files
# (zip_path <- 'input_data/nhgis0060_shape')
(zip_path <- '../biodiversity_in_a_concrete_jungle_data_too_big/wetlands/zipped_wetlands')
(zip_files<- list.files(zip_path, full.names = TRUE))
(out_path <- '../biodiversity_in_a_concrete_jungle_data_too_big/wetlands/')

# # done once, no need to re-do
# tic(); for(i in zip_files){
#   print(i)
#   unzip(i, exdir = out_path)
#   file.remove(i)
# }; toc() # ~3 mins

# # remove the zips?
# file.remove('../biodiversity_in_a_concrete_jungle_data_too_big/wetlands/zipped_wetlands')

```

## B shapefiles - no need to run this again.
```{r eval=FALSE, include=FALSE}

list.files('../biodiversity_in_a_concrete_jungle_data_too_big/wetlands/zipped_wetlands')

# # get zip files
# (zip_path <- 'input_data/nhgis0060_shape')
(zip_path <- '../biodiversity_in_a_concrete_jungle_data_too_big/wetlands/zipped_wetlands')
(zip_files<- list.files(zip_path, full.names = TRUE))
(out_path <- '../biodiversity_in_a_concrete_jungle_data_too_big/wetlands/')

# done once, no need to re-do
tic(); for(i in zip_files){
  print(i)
  unzip(i, exdir = out_path)
  file.remove(i)
}; toc() # ~3 mins

# remove the zips? Yes.
file.remove('../biodiversity_in_a_concrete_jungle_data_too_big/wetlands/zipped_wetlands')

# remove unnecesary files..
# Wetlands AND NOT HISTORIC OR RIPARIAN
# 
# Yes
# Wetlands
# Riparian
# 
# No
# Project
# Metadata
# Historic

# # tibble of all of the contents
# (
#   shape_names <- 
#     tibble(filename = list.files('../biodiversity_in_a_concrete_jungle_data_too_big/wetlands/'
#                                  , recursive = TRUE
#                                  ))
#   )
# 
# # list of things to keep
# (
#   files_to_keep <- # will reduce to just *.shp in a second
#     shape_names |> #View()
#     filter(str_detect(filename, paste(c("Wetlands", "Riparian"), collapse = '|'))) |> # has Either wetlands or riparian
#     filter(str_detect(filename, "Historic", negate = TRUE)) |> # drop historic
#     filter(str_detect(filename, "Project", negate = TRUE)) |> # view() # drop project
#     filter(str_detect(filename, "Metadata", negate = TRUE)) |> # view() # drop project
#     pull(filename)
#   )
# 
# length(files_to_keep) / 8 #:-/
# 
# files_to_remove <- 
#   shape_names |> 
#   filter(filename %nin% files_to_keep)
# 
# files_to_remove |> View() # looks good.
# 
# for(i in files_to_remove){
#   print(i)
#   file.remove(paste0('../biodiversity_in_a_concrete_jungle_data_too_big/wetlands/', i))
# }
# 
# rm(files_to_keep, files_to_remove) # clean up

```


# 3 examine
## A gdb - defunct
```{r eval=FALSE, include=FALSE}
list.files('../biodiversity_in_a_concrete_jungle_data_too_big/wetlands/')
st_layers('../biodiversity_in_a_concrete_jungle_data_too_big/wetlands/WV_geodatabase_wetlands.gdb')

slug <- '../biodiversity_in_a_concrete_jungle_data_too_big/wetlands/WV_geodatabase_wetlands.gdb'
wv_nwi <- 
  st_read(
    slug
    # , 'WV_Wetlands_Project_Metadata' # grid with meta
    # , 'WV_Wetlands_Historic_Map_Info'  # less-extensive grid, with meta
    # , 'West_Virginia' # state outline w grids.
    , 'WV_Wetlands'
  )

# 2x checks
wv_nwi |> glimpse()
wv_nwi |> 
  st_drop_geometry() |> 
  tabyl(WETLAND_TYPE)

wv_nwi |> 
  sample_n(1000) |> 
  mapview()

```

## B shp
```{r}


shapes_to_iterate_over <- 
  tibble(filenames = 
           list.files('../biodiversity_in_a_concrete_jungle_data_too_big/wetlands/'
                      , recursive = TRUE
                      , pattern = 'shp$'
                      , full.names = TRUE
                      )
         ) |> 
  mutate(state = str_remove_all(filenames, '../biodiversity_in_a_concrete_jungle_data_too_big/wetlands//') |>
           str_sub(1,2)
         , item = str_remove_all(filenames, '../biodiversity_in_a_concrete_jungle_data_too_big/wetlands//') |>
           str_remove_all('shapefile_wetlands') |>
           str_remove_all('.shp') |>
           str_remove('.._\\/')
         )

shapes_to_iterate_over$item

random_shape <- 
  shapes_to_iterate_over |> 
  sample_n(1) |>
  mutate(shp = map(filenames, ~st_read(.))) |> 
  unnest(shp)


# 2x checks
random_shape |> glimpse()
random_shape |> 
  st_drop_geometry() |> 
  tabyl(WETLAND_TY)

random_shape |> 
  st_as_sf() |> 
  sample_n(1000) |> 
  mapview()

```


# test another state out.. ugh overlaps, large duplicative area - what to do? Clipping geom reduces to state-specific union of MSA, UA, and HOLC boundaries
```{r eval=FALSE, include=FALSE}
list.files('../biodiversity_in_a_concrete_jungle_data_too_big/wetlands/')
st_layers('../biodiversity_in_a_concrete_jungle_data_too_big/wetlands/MD_geodatabase_wetlands.gdb')

slug <- '../biodiversity_in_a_concrete_jungle_data_too_big/wetlands/MD_geodatabase_wetlands.gdb'
md_nwi <- 
  st_read(
    slug
    # , 'WV_Wetlands_Project_Metadata' # grid with meta
    # , 'WV_Wetlands_Historic_Map_Info'  # less-extensive grid, with meta
    # , 'Maryland' # state outline w grids.
    , 'WV_Wetlands'
  )

# 2x checks
md_nwi |> glimpse()
md_nwi |> 
  st_drop_geometry() |> 
  tabyl(WETLAND_TYPE)

md_nwi |> 
  sample_n(1000) |> 
  mapview()

```



# 4 clipping geometry - old outdated
```{r eval=FALSE, include=FALSE}

sf_use_s2(FALSE) # suppresses errors, allows st_erase to run


nwi_crs <- 
  st_read(
    '../biodiversity_in_a_concrete_jungle_data_too_big/wetlands/WV_geodatabase_wetlands.gdb'
    , 'WV_Wetlands'
  ) |> st_crs()


list.files('working_data/holc_polys_saved')

msa  <- st_read('working_data/MSA/msa_as_geopackage_2022-12-08 15-40-07.gpkg') |> select() |> summarise()
ua   <- st_read('working_data/UA/UA_as_geopackage_2023-01-23 15-48-08.gpkg') |> select() |> summarise()
holc <- st_read('working_data/holc_polys_saved/holc_plys_2022-12-08 20-13-17.gpkg') |> select() |> summarise()


clipper_geom_temp  <- st_union(msa, ua)
clipper_geom       <- st_union(clipper_geom_temp, holc) |> st_transform(crs = nwi_crs)

clipper_geom |> mapview()

# clean things up
rm(msa, ua, holc)
rm(clipper_geom_temp)



```


# 4 clipping geometry - by state
## A get state boundaries
```{r}

(state_boundaries <- 
  get_acs(geography = 'state'
          , variables = "B19013_001"
          , state = states$value
          , year = 2020
          , geometry = TRUE) |> 
  st_transform(crs = st_crs(msa)) |> 
  select(census_name = NAME))

```


## B consolidate geometries to clip NWI down to: MSA, UA, HOLC
```{r}

sf_use_s2(FALSE) # suppresses errors, allows overlays to run


nwi_crs <- random_shape |> st_as_sf() |> st_crs()


# list.files('working_data/holc_polys_saved')


msa_state  <- 
  st_read('working_data/MSA/msa_as_geopackage_2022-12-08 15-40-07.gpkg') |> 
  select(msa_name) |> 
  mutate(msa_name = str_remove(msa_name,  ' Metro Area') |> 
           str_remove(' Micro Area')) |> 
  separate(msa_name, into = c('msa_text', 'state'), sep = ', ') |> 
  separate(state, into = c('state1', 'state2', 'state3', 'state4'), sep = '-') |> 
  pivot_longer(cols = starts_with('state'), values_to = 'state') |> 
  drop_na(state) |> 
  group_by(state) |> 
  summarise() |> 
  st_intersection(state_boundaries) |> 
  group_by(census_name) |> 
  summarise()

# look
msa_state |> mapview()

# # do MSA's fully contain HOLC polygons
# something is wrong with the HOLC polygons!
# tic(); test_overlap <- 
#   st_parallel(
#       msa_state
#     , st_difference
#     , 6
#     , holc
#     ); toc()


# # repeat the breaking up
ua_state <-
  st_read('working_data/UA/UA_as_geopackage_2023-01-23 15-48-08.gpkg') |> 
  select(ua_name = NAME) |> 
  mutate(ua_name = str_remove(ua_name, ' Urbanized Area \\(2010\\)') |> 
           str_remove(' Urban Cluster \\(2010\\)') |> 
           str_replace_all('--', ',')) |> 
  separate(ua_name, into = c('ua_text', 'state'), sep = ', ') |> 
  separate(state, into = c('state1', 'state2', 'state3', 'state4')) |> 
  pivot_longer(cols = starts_with('state'), values_to = 'state') |> 
  drop_na(state) |> 
  group_by(state) |> 
  summarise() |> 
  st_intersection(state_boundaries) |> 
  group_by(census_name) |> 
  summarise()

msa_state |> mapview() + mapview(ua_state) # very nice


tic(); (
  clipper_geom <- 
    msa_state |> 
    st_cast('MULTIPOLYGON') |> # why?
    bind_rows(ua_state) |> 
    group_by(census_name) |> 
    summarise() |> 
    st_transform(crs = nwi_crs) |> 
    left_join(
      fips_codes |> distinct(state, state_name)
      , by = c('census_name' = 'state_name')
      ) |> 
    select(census_name, state) |> 
    # st_simplify() |> # NEW
    st_make_valid() |> # NEW
    filter(st_is(geom, c('POLYGON', 'MULTIPOLYGON'))) # NEW
  ); toc() # ~3 seconds

clipper_geom |> 
  # slice(1) |> 
  mapview()

clipper_geom

# clean things up
rm(msa_state, ua_state)
rm(msa)




```


# 5 bring in a state, clip its water down, save out (just cuz), and summarize per holc poly, UA, and MSA
```{r}


rm(random_shape)


holc <-
  st_read('working_data/holc_polys_saved/holc_plys_2022-12-08 20-13-17.gpkg') |> 
  st_make_valid()

ua_ungraded <- 
  st_read('working_data/UA_donut/UA_ungraded_2023-01-23 16-05-42.gpkg') |> 
  select(ua_GEOID = GEOID) |> 
  st_transform(crs = st_crs(holc))

msa_ungraded <- 
  st_read('working_data/MSA_donut/msa_ungraded.gpkg') |> 
  select(msa_GEOID) |> 
  st_transform(crs = st_crs(holc))


parallelly::availableCores() # 16, yeah!
ncore <- 8  # How many do you want to use?
# ncore <- 4  # How many do you want to use?

set.seed(1); gc()

tic(); for(i in shapes_to_iterate_over$filenames){
# tic(); for(i in shapes_to_iterate_over$filenames[20]){ # for testing a non-overlap
# tic(); for(i in shapes_to_iterate_over$filenames[58:59]){ # skip WI_Wetlands_North?
# tic(); for(i in shapes_to_iterate_over$filenames[57]){ # WI_Wetlands_North ONLY? - ack broken file

# # WISCONSON is bad
# tic(); for(i in shapes_to_iterate_over$filenames[57]){ # WI_Wetlands_North ONLY? - ack broken file
# i <- '../biodiversity_in_a_concrete_jungle_data_too_big/wetlands/WI_geodatabase_wetlands.gdb/'
# st_code <- 'WI'
# item <- 'WI_Wetlands'
# nwi <- st_read(i, item) |> 
#   select(wetland_type = WETLAND_TYPE)

  tic()
  print('==============================================================================================')
  print(i)
  
  # get state code
  st_code <-
    shapes_to_iterate_over |> 
    filter(filenames == i) |> 
    pull(state)
  
  # get 'item' for naming
  item <-
    shapes_to_iterate_over |> 
    filter(filenames == i) |> 
    pull(item)
  
  # read in wetlands/riparian area
  tic(); nwi <- 
    st_read(i
            , query = paste0('SELECT WETLAND_TY as wetland_type FROM ', item) # needed?
            ); toc(); beep()
  
  # reduce clipper to state
  (clipper_geom_temp <- clipper_geom |> filter(state == st_code))
  
  # intersect item per state
  tic(); clipped_nwi <- 
    st_parallel(
        nwi
      , st_intersection
      , ncore
      , clipper_geom_temp # pre-selection, saves SOOO much time.
      ) |> 
    select(wetland_type, state) |> 
    filter(st_is(geometry, c('POLYGON', 'MULTIPOLYGON'))) # drop linestrings, if any
    # st_cast('POLYGON') # causing more warnings then preventing?
  
  # free up some memory
  rm(nwi)
  
  # IF THERE IS OVERLAP
  # write out clipped parquet file
  if(nrow(clipped_nwi) > 0){
    clipped_nwi |> 
      st_write_parquet(paste0('../biodiversity_in_a_concrete_jungle_data_too_big/wetlands_clipped/'
                              , item
                              , '_nwi.parquet')); toc(); beep()
    
    # reproject clipped nwi to match holc polys
    tic(); clipped_nwi <- 
      clipped_nwi |> 
      st_transform(crs = st_crs(holc)); toc(); beep()
    
    # read in reduced holc polygons
    holc_temp <- 
      holc |>
      select(id, state) |> 
      filter(state == st_code) |> 
      select(id)
    
    # intersect clipped nwi with holc polygons, calculate overlap per poly AND per type of wetland
    if(nrow(clipped_nwi) < ncore){
      holc_nwi_int <- 
        clipped_nwi |> 
        st_intersection(holc_temp) %>% 
        mutate(area_nwi_km2 = as.double(st_area(.) / 1000000)) |> 
        st_drop_geometry() |> 
        group_by(id, wetland_type) |> # holc polygon AND by wetland type
        summarise(sum_area_nwi_km2 = sum(area_nwi_km2, na.rm = TRUE))
    } else{
      tic(); holc_nwi_int <- 
        st_parallel(
            clipped_nwi
          , st_intersection
          , ncore
          , holc_temp
          ) %>%
        mutate(area_nwi_km2 = as.double(st_area(.) / 1000000)) |> 
        st_drop_geometry() |> 
        group_by(id, wetland_type) |> # holc polygon AND by wetland type
        summarise(sum_area_nwi_km2 = sum(area_nwi_km2, na.rm = TRUE)); toc()
      }
    
    
    # IF there is a holc-nwi intersection, save it out
    if(nrow(holc_nwi_int) > 0){
      holc_nwi_int |> 
      write_csv(paste0('../biodiversity_in_a_concrete_jungle_data_too_big/wetlands_int/holc_'
                       , item
                       , '_nwi.csv')); beep()
      }; rm(holc_nwi_int)
    
    
    # URBAN AREA polygons, calculate overlap per poly AND per type of wetland
    if(nrow(clipped_nwi) < ncore){ # could have been combined with above
      ua_nwi_int <- 
        clipped_nwi |> 
        st_intersection(ua_ungraded) %>%
        mutate(area_nwi_km2 = as.double(st_area(.) / 1000000)) |> 
        st_drop_geometry() |> 
        group_by(ua_GEOID, wetland_type) |> # ua polygon AND by wetland type
        summarise(sum_area_nwi_km2 = sum(area_nwi_km2, na.rm = TRUE))
    } else{
      tic(); ua_nwi_int <- 
        st_parallel(
            clipped_nwi
          , st_intersection
          , ncore
          , ua_ungraded
          ) %>%
        mutate(area_nwi_km2 = as.double(st_area(.) / 1000000)) |> 
        st_drop_geometry() |> 
        group_by(ua_GEOID, wetland_type) |> # ua polygon AND by wetland type
        summarise(sum_area_nwi_km2 = sum(area_nwi_km2, na.rm = TRUE)); toc()
    }
    
    # IF there is a ua-nwi intersection, save it out
    if(nrow(ua_nwi_int) > 0){
      ua_nwi_int |> 
      write_csv(paste0('../biodiversity_in_a_concrete_jungle_data_too_big/wetlands_int/ua_'
                       , item
                       , '_nwi.csv')); beep()
      }; rm(ua_nwi_int)
    
    # MSA polygons, calculate overlap per poly AND per type of wetland
    if(nrow(clipped_nwi) < ncore){ # could have been combined with above
      msa_nwi_int <- 
        clipped_nwi |> 
        st_intersection(msa_ungraded) %>%
        mutate(area_nwi_km2 = as.double(st_area(.) / 1000000)) |> 
        st_drop_geometry() |> 
        group_by(msa_GEOID, wetland_type) |> # ua polygon AND by wetland type
        summarise(sum_area_nwi_km2 = sum(area_nwi_km2, na.rm = TRUE))
    } else{
    tic(); msa_nwi_int <-
      st_parallel(
          clipped_nwi
        , st_intersection
        , ncore
        , msa_ungraded
        ) %>%
      mutate(area_nwi_km2 = as.double(st_area(.) / 1000000)) |> 
      st_drop_geometry() |> 
      group_by(msa_GEOID, wetland_type) |> # msa polygon AND by wetland type
      summarise(sum_area_nwi_km2 = sum(area_nwi_km2, na.rm = TRUE)); toc()
    }
    
    # IF there is an msa-nwi intersection, save it out
    if(nrow(msa_nwi_int) > 0){
      msa_nwi_int |> 
      write_csv(paste0('../biodiversity_in_a_concrete_jungle_data_too_big/wetlands_int/msa_'
                       , item
                       , '_nwi.csv')); beep()
    }; rm(msa_nwi_int)
    
    }; toc(); beep() # ends first if with no overlapping clipped_nwi
  
  rm(clipped_nwi); gc()
  }; toc()


```

# remove the large files ARE YOU SURE YOU WANT TO DO THIS?!?!?
```{r eval=FALSE, include=FALSE}

# list.files('../biodiversity_in_a_concrete_jungle_data_too_big/wetlands')
# file.remove('../biodiversity_in_a_concrete_jungle_data_too_big/wetlands/')
# for(i in list.files('../biodiversity_in_a_concrete_jungle_data_too_big/wetlands'
#                     , full.names = TRUE
#                     , recursive = TRUE)){
#   file.remove(i)
# }

```

start here!
# 6 pull HOLC, UA, MSA summaries together
```{r}
# nwi_long <- 
#   tibble(filenames = 
#            list.files('../biodiversity_in_a_concrete_jungle_data_too_big/wetlands_int/'
#                       , full.names = TRUE
#                       )) |> 
#   mutate(file = map(filenames, ~read_csv(., col_types = 'ccd'))) |> # define types!
#   select(-filenames) |> 
#   unnest(file) |> 
#   select(id, ua_GEOID, msa_GEOID, everything()) |> # just reorder columns: keys then vals
#   pivot_longer(cols = c(id, ua_GEOID, msa_GEOID)) |> 
#   group_by(name, value, wetland_type) |> # squish down because of overlaps
#   summarise(sum_area_nwi_km2 = sum(sum_area_nwi_km2)) |> 
#   ungroup() 
#   
# 
# nwi_wide <- 
#   nwi_long |> 
#   pivot_wider(id_cols = c(name, value)
#               , names_from = wetland_type
#               , values_from = sum_area_nwi_km2
#               , values_fill = 0
#               ) |> 
#   mutate(
#     nwi_total_area_km2 = rowSums(across(where(is.double)))
#   )
# 
# # save out
# nwi_wide |> 
#   write_csv('working_data/nwi/nwi_wide.csv')
# 
# nwi_long |> 
#   write_csv('working_data/nwi/nwi_long.csv')

# read in NWI summaries
nwi_wide <- read_csv('working_data/nwi/nwi_wide.csv')

nwi_long <- read_csv('working_data/nwi/nwi_long.csv')

# ~log normal totals
nwi_wide |> 
  ggplot(aes(nwi_total_area_km2)) +
  geom_density() +
  scale_x_log10() + 
  facet_wrap(~name) +
  NULL

# ~log normal by type
nwi_long |> 
  ggplot(aes(sum_area_nwi_km2)) +
  geom_density() +
  scale_x_log10() + 
  facet_grid(name ~ wetland_type) +
  theme_bw(16) +
  NULL

# NWI by HOLC
holc <-
  st_read('working_data/holc_polys_saved/holc_plys_2022-12-08 20-13-17.gpkg') |> 
  st_make_valid()

holc_nwi <- 
  holc |> 
  left_join(
    nwi_wide |> 
      filter(name == 'id') |> 
      rename(id = value) |> 
      select(-name)
    , by = 'id'
      ) %>%
  replace(is.na(.), 0)
```

## A VIZ
### i cover
```{r}


holc_nwi |> 
  ggplot(aes(holc_grade, nwi_total_area_km2)) +
  geom_boxplot() + 
  scale_y_log10()

my_comparisons <- list(c("A", "B"), c("A", "C"), c("A", "D")
                       # , c("A", "UA"), c("A", "MSA")
                       )

holc_nwi |> 
  ggboxplot(x = 'holc_grade', y = 'nwi_total_area_km2'
            , palette = holc_pal_f
            , fill = 'holc_grade'
             # , width = 0.12
            # removing outliers
            , outlier.color = NA
            , ggtheme = theme_pubr(base_size = 16)) +
  stat_compare_means(comparisons = my_comparisons
                     , method = 'wilcox.test'
                     , symnum.args = list(cutpoints = c(0, 0.0001, 0.001, 0.01, 0.05, Inf)
                                          , symbols = c("****", "***", "**", "*", "ns"))
                     ) +  # Add pairwise comparisons p-value
  # scale_y_sqrt(labels = scales::label_log(digits = 2)) +
  scale_y_log10(labels = scales::label_log(digits = 2)) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = .5), legend.position = c(.9, .25)) +
  labs(
    x = '', y = 'total National Wetlands Inventory cover (km^2)') +
  annotation_logticks(sides = 'l') +
  # facet_wrap(~taxon, scales = 'free', nrow = 2) +
  NULL


# PERCENT cover
holc_nwi |> 
  mutate(p_nwi = 100*(nwi_total_area_km2 / area_holc_km2)) |> 
  ggboxplot(x = 'holc_grade', y = 'p_nwi'
            , palette = holc_pal_f
            , fill = 'holc_grade'
             # , width = 0.12
            # removing outliers
            , outlier.color = NA
            , ggtheme = theme_pubr(base_size = 16)) +
  stat_compare_means(comparisons = my_comparisons
                     , method = 'wilcox.test'
                     , symnum.args = list(cutpoints = c(0, 0.0001, 0.001, 0.01, 0.05, Inf)
                                          , symbols = c("****", "***", "**", "*", "ns"))
                     ) +  # Add pairwise comparisons p-value
  # scale_y_sqrt(labels = scales::label_log(digits = 2)) +
  scale_y_log10(labels = scales::label_log(digits = 2)) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = .5), legend.position = c(.9, .25)) +
  labs(
    x = '', y = 'NWI coverage (% of polygon)') +
  annotation_logticks(sides = 'l') +
  # facet_wrap(~taxon, scales = 'free', nrow = 2) +
  NULL

# ggsave(  filename = paste0(getwd(), '/figures/p_NWI_', Sys.Date(), '.png')
#        # , width = 8.7, height = 10, units = 'cm'
#        , dpi = 450
#        , scale = 1.65)



```

### ii cover by subtype
```{r}

 
my_comparisons <- list(c("A", "B"), c("A", "C"), c("A", "D")
                       # , c("A", "UA"), c("A", "MSA")
                       )


# PERCENT cover
holc_nwi |> 
  st_drop_geometry() |> 
  # https://dplyr.tidyverse.org/reference/across.html
  mutate(across(`Freshwater Forested/Shrub Wetland` : nwi_total_area_km2, ~100*(.x / area_holc_km2))) |> 
  select(id, holc_grade, `Freshwater Forested/Shrub Wetland` : nwi_total_area_km2) |> 
  pivot_longer(`Freshwater Forested/Shrub Wetland` : nwi_total_area_km2) |> 
  ggboxplot(x = 'holc_grade', y = 'value'
            , palette = holc_pal_f
            , fill = 'holc_grade'
             # , width = 0.12
            # removing outliers
            , outlier.color = NA
            # , ggtheme = theme_pubr(base_size = 16)
            ) +
  stat_compare_means(comparisons = my_comparisons
                     , method = 'wilcox.test'
                     , symnum.args = list(cutpoints = c(0, 0.0001, 0.001, 0.01, 0.05, Inf)
                                          , symbols = c("****", "***", "**", "*", "ns"))
                     ) +  # Add pairwise comparisons p-value
  # scale_y_sqrt(labels = scales::label_log(digits = 2)) +
  scale_y_log10(labels = scales::label_log(digits = 2)) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = .5), legend.position = c(.1, .25)) +
  labs(
    x = '', y = 'NWI coverage (% of polygon)') +
  annotation_logticks(sides = 'l') +
  facet_wrap(~name, scales = 'free', nrow = 2) +
  NULL

# ggsave(  filename = paste0(getwd(), '/figures/p_NWI_types_', Sys.Date(), '.png')
#        # , width = 8.7, height = 10, units = 'cm'
#        , dpi = 450
#        , scale = 1.65)



```


### iii NYC only
```{r}

 
my_comparisons <- list(c("A", "B"), c("A", "C"), c("A", "D")
                       # , c("A", "UA"), c("A", "MSA")
                       )

# PERCENT cover
nyc_cities <- c('Bronx', 'Brooklyn', 'Manhattan', 'Queens', 'Staten Island')
holc_nwi |> 
  st_drop_geometry() |> 
  filter(city %in% nyc_cities) |> # tabyl(city)
  # https://dplyr.tidyverse.org/reference/across.html
  mutate(across(`Freshwater Forested/Shrub Wetland` : nwi_total_area_km2, ~100*(.x / area_holc_km2))) |> 
  select(id, holc_grade, `Freshwater Forested/Shrub Wetland` : nwi_total_area_km2) |> 
  pivot_longer(`Freshwater Forested/Shrub Wetland` : nwi_total_area_km2) |> 
  ggboxplot(x = 'holc_grade', y = 'value'
            , palette = holc_pal_f
            , fill = 'holc_grade'
             # , width = 0.12
            # removing outliers
            , outlier.color = NA
            # , ggtheme = theme_pubr(base_size = 16)
            ) +
  stat_compare_means(comparisons = my_comparisons
                     , method = 'wilcox.test'
                     , symnum.args = list(cutpoints = c(0, 0.0001, 0.001, 0.01, 0.05, Inf)
                                          , symbols = c("****", "***", "**", "*", "ns"))
                     ) +  # Add pairwise comparisons p-value
  # scale_y_sqrt(labels = scales::label_log(digits = 2)) +
  scale_y_log10(labels = scales::label_log(digits = 2)) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = .5), legend.position = c(.1, .25)) +
  labs(
    x = '', y = 'NWI coverage (% of polygon)') +
  annotation_logticks(sides = 'l') +
  facet_wrap(~name, scales = 'free', nrow = 2) +
  NULL

# ggsave(  filename = paste0(getwd(), '/figures/p_NWI_types_NYC_', Sys.Date(), '.png')
#        # , width = 8.7, height = 10, units = 'cm'
#        , dpi = 450
#        , scale = 1.65)



```



old and ewwwey - busted geometries in shapefiles
```{r}
# tic(); for(i in states[c(1, 2),]){ # for testing
# tic(); for(i in states$value){
# tic(); for(i in states$value[c(32: 40)]){ # MI, MS, NY failed
# tic(); for(i in states$value){
# tic(); for(i in states$value[c((16:40))]){ # MI failed
# tic(); for(i in states$value[c((40:19))]){ # SC failed
  tic()
  print('==============================================================================================')
  print(i)
  
  # TODO update
  path <- paste0('../biodiversity_in_a_concrete_jungle_data_too_big/wetlands/', i, '_geodatabase_wetlands.gdb')
  
  # # OG    
  # query = 'SELECT * FROM \"combined_blocks_2020_no_water_single_part\" WHERE GISJOIN = \"G06003702060102000\"'
  
  # tic(); nwi <- 
  #   st_read(path, paste0(i, '_Wetlands')) |> 
  #   select(attribute = ATTRIBUTE, wetland_type = WETLAND_TYPE) |> 
  #   rowid_to_column(); toc()
  # 
  tic(); nwi <- 
    st_read(path
            # , query = paste0('SELECT attribute = ATTRIBUTE, WETLAND_TYPE FROM ', i, '_Wetlands limit 10') # for testing
            , query = paste0('SELECT ATTRIBUTE as attribute, WETLAND_TYPE as wetland_type FROM ', i, '_Wetlands')
            ) |> 
    # st_as_sf() |> # change type
    rowid_to_column(); toc(); beepr::beep() # ~2 mins for SC
  
  tic(); check <- nwi |> st_is_valid(reason = TRUE); toc(); beepr::beep() # ~11 mins for SC
  
  # tic(); nwi <-
  #   st_read(path
  #           , promote_to_multi = FALSE
  #           , as_tibble = TRUE
  #           , query = paste0('SELECT attribute = ATTRIBUTE, WETLAND_TYPE FROM ', i, '_Wetlands limit 11000') # for testing
  #           # , query = paste0('SELECT ATTRIBUTE as attribute, WETLAND_TYPE as wetland_type FROM ', i, '_Wetlands')
  #           ) |>
  #   # st_make_valid(); toc(); beepr::beep()
  #   mutate(Shape = st_as_sfc(geos::geos_make_valid(Shape))); toc(); beepr::beep() # ~2 mins for SC
  #   # mutate(Shape = st_as_sfc(Shape)
#            
# test <- nwi |> sample_n(1000) |> st_cast('MULTIPOLYGON') |> mutate(Shape = st_as_sfc(geos::geos_make_valid(Shape)))
  
  # table(check)
  
  invalids <- 
    check |> 
    tibble() |> 
    rowid_to_column() |> 
    filter(check != 'Valid Geometry')
  
  # pull out the broken ones and fix
  tic(); nwi_fix <-
    nwi |> 
    filter(rowid %in% invalids$rowid) |> 
    st_make_valid() |> 
    st_buffer(0); toc()#|> mapview()
  
  # set working ones aside
  tic(); nwi_valid <- 
    nwi |> 
    anti_join(invalids, by = 'rowid'); toc() # ~12 mins
  
  # # drop the invalids, bind the fixed invalids back on
  # tic(); nwi <-
  #   nwi |> 
  #   anti_join(invalids, by = 'rowid') |> 
  #   # filter(rowid %nin% invalids$rowid) |> # NOT IN
  #   bind_rows(nwi_fix); toc()

  beepr::beep()
  
  # reduce clipper
  clipper_geom_temp <- clipper_geom |> filter(state == i)
  
  # intersect working ones
  tic(); nwi_int <- 
    st_parallel(
      nwi
        # nwi_valid |> st_make_valid()
      , st_intersection
      , ncore
      , clipper_geom_temp # pre-selection, saves SOOO much time.
      ) |> 
    # mutate(area_nwi_km2 = as.double(st_area(.) / 1000000)) |> 
    select(wetland_type, state) |> 
    filter(st_is(geometry, c('POLYGON', 'MULTIPOLYGON'))); toc(); beepr::beep() # drop linestrings, if any
  
  # intersect previously broken ones
    tic(); nwi_int_fix <- 
    st_parallel(
      # nwi
        nwi_fix
      , st_intersection
      , ncore
      , clipper_geom_temp # pre-selection, saves SOOO much time.
      ) |> 
    # mutate(area_nwi_km2 = as.double(st_area(.) / 1000000)) |> 
    select(wetland_type, state) |> 
    filter(st_is(Shape, c('POLYGON', 'MULTIPOLYGON'))) |> 
      st_cast('POLLYGON'); toc(); beepr::beep() # drop linestrings, if any
    
    # combine
    nwi_int |> 
      bind_rows(nwi_int_fix) |> 
      st_write_parquet(paste0('../biodiversity_in_a_concrete_jungle_data_too_big/wetlands_clipped/', i, '_nwi.parquet')); toc()
    # st_write(paste0('../biodiversity_in_a_concrete_jungle_data_too_big/wetlands_clipped/', i, '_nwi.gpkg')); toc()
  
  beepr::beep()
  
  clipped_nwi <- 
    st_read_parquet(
      paste0('../biodiversity_in_a_concrete_jungle_data_too_big/wetlands_clipped/', i, '_nwi.parquet')
    # st_read(
    #   paste0('../biodiversity_in_a_concrete_jungle_data_too_big/wetlands_clipped/', i, '_nwi.gpkg')
    ) |> 
    st_transform(crs = st_crs(holc))
  
  beepr::beep()

  holc_temp <- 
    holc |> 
    # st_read('working_data/holc_polys_saved/holc_plys_2022-12-08 20-13-17.gpkg') |> 
    select(id, state) |> 
    filter(state == i) |> 
    select(id)
  
  beepr::beep()
  
  st_parallel(
      clipped_nwi
    , st_intersection
    , ncore
    , holc_temp
    ) %>%
    mutate(area_nwi_km2 = as.double(st_area(.) / 1000000)) |> 
    st_drop_geometry() |> 
    group_by(id) |> 
    summarise(sum_area_nwi_km2 = sum(area_nwi_km2, na.rm = TRUE)) |> 
    write_csv(paste0('../biodiversity_in_a_concrete_jungle_data_too_big/wetlands_holc_int/', i, '_nwi.csv'))

  toc()
  beepr::beep()
  gc()
  }; toc(); beepr::beep()


# nwi_cliped
# nwi_cliped |> glimpse()
# nwi_cliped |> sample_n(1000) |> mapview()



```







