---
title: "whats up with wetlands?"
author: "Dexter H. Locke, PhD"
date: "`r format(Sys.time())`"
output: html_document
editor_options: 
  chunk_output_type: console
---

# rasterize these polygons
#     fasterize
# then summarize the pixels per polygon


https://www.fws.gov/program/national-wetlands-inventory/download-state-wetlands-data

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# 0 set up: load libraries, custom functions, set defaults
```{r}

# load libraries
# packages we'll be using
packs <- c(
    'tidyverse'  # a must have
  , 'tidylog'    # prints out what was done in dplyr and tidr
  # , 'gbifdb' # GBIF
  # , 'fst' # a faster table, makes outputs files much smaller, too.
  # , 'terra'
  # , 'KnowBR'    # creates biodiversity estimates like completeness.
  , 'tidycensus'      # Census access
  , 'sf'        # spatial support
  , 'mapview'   # webmaps
  , 'janitor'   # cleans things up, also pipe-friendly cross-tabulations
  , 'tictoc'    # times things
  , 'beepr'     # makes noises
  , 'sfarrow'
)


# check for all of the libraries, install if you don't have them
if (length(setdiff(packs, rownames(installed.packages()))) > 0) {
  install.packages(setdiff(packs, rownames(installed.packages())))  
}

# load them
vapply(packs, library, character.only = TRUE, logical(1), logical.return = TRUE, quietly = TRUE)



# custom function for "Not In"
`%nin%` <- Negate(`%in%`)


# redlining colors
holc_pal <- c('#92BC6B' # green
              , '#92C7C9' # blue
              , '#E7DC6B' # yellow
              , '#E47D67' # red
              #, '#A9A9A9'
              ) # dark gray)

holc_pal_f<- c('#92BC6B' # green
              , '#92C7C9' # blue
              , '#E7DC6B' # yellow
              , '#E47D67' # red
              , '#A9A9A9'
              , '#000000')

# fixes mapview
mapviewOptions(fgb = FALSE)


# Paralise any simple features analysis.
# https://www.spatialanalytics.co.nz/post/2018/04/01/fixing-st-par/
# define
st_parallel <- function(sf_df, sf_func, n_cores, ...){

  # Create a vector to split the data set up by.
  split_vector <- rep(1:n_cores, each = nrow(sf_df) / n_cores, length.out = nrow(sf_df))

  # Perform GIS analysis
  split_results <- split(sf_df, split_vector) %>%
    parallel::mclapply(function(x) sf_func(x, ...), mc.cores = n_cores)
  
  
  # Define the output_class. If length is greater than two, then grab the second variable.
  output_class <- class(split_results[[1]])
  if (length(output_class) == 2){
    output_class <- output_class[2]
  }
  
  # Combine results back together. Method of combining depends on the output from the function.
  if (output_class == "matrix"){
    result <- do.call("rbind", split_results)
    names(result) <- NULL
  } else if (output_class == "sfc") {
    result <- do.call("c", split_results)
    result <- sf_func(result) # do.call combines the list but there are still n_cores of the geometry which had been split up. Running st_union or st_collect gathers them up into one, as is the expected output of these two functions. 
  } else if (output_class %in% c('list', 'sgbp') ){
    result <- do.call("c", split_results)
    names(result) <- NULL
  } else if (output_class == "data.frame" ){
    result <- do.call("rbind", split_results)
  } else {
    stop("Unknown class. st_parallel only accepts the following outputs at present: sfc, list, sf, matrix, sgbp.")
  }
  
  # Return result
  return(result)
}
```

# 1 which states do we need to download?
```{r}

# pull from msa
msa <- st_read('working_data/MSA/msa_as_geopackage_2022-12-08 15-40-07.gpkg')

states_msa <- 
  msa |> 
  st_drop_geometry() |> 
  # tibble() |> 
  select(msa_name) |> 
  mutate(msa_name = str_remove(msa_name,  ' Metro Area') |> 
           str_remove(' Micro Area')) |> 
  separate(msa_name, into = c('trash', 'state'), sep = ', ') |> 
  separate(state, into = c('state1', 'state2', 'state3', 'state4'), sep = '-') |> 
  select(starts_with('state')) |> 
  rowid_to_column() |> 
  pivot_longer(-rowid) |> 
  distinct(value) |> 
  filter(!is.na(value)) |> 
  arrange(value) |> data.frame()# 40

(states <- 
  st_read('working_data/holc_polys_saved/holc_plys_2022-12-08 20-13-17.gpkg') |> 
  st_drop_geometry() |> 
  distinct(state)) # 38

# go with MSA-derived states!
states_msa |> left_join(states, by = c('value' = 'state'), keep = TRUE)

(states <- states_msa); rm(states_msa)
```

# 2 unzip
## A geodatabase - defunct, did not work (erroneous geometries)
```{r eval=FALSE, include=FALSE}

list.files('../biodiversity_in_a_concrete_jungle_data_too_big/wetlands/zipped_wetlands')

# # get zip files
# (zip_path <- 'input_data/nhgis0060_shape')
(zip_path <- '../biodiversity_in_a_concrete_jungle_data_too_big/wetlands/zipped_wetlands')
(zip_files<- list.files(zip_path, full.names = TRUE))
(out_path <- '../biodiversity_in_a_concrete_jungle_data_too_big/wetlands/')

# # done once, no need to re-do
# tic(); for(i in zip_files){
#   print(i)
#   unzip(i, exdir = out_path)
#   file.remove(i)
# }; toc() # ~3 mins

# # remove the zips?
# file.remove('../biodiversity_in_a_concrete_jungle_data_too_big/wetlands/zipped_wetlands')

```

## B shapefiles - no need to run this again.
```{r eval=FALSE, include=FALSE}

list.files('../biodiversity_in_a_concrete_jungle_data_too_big/wetlands/zipped_wetlands')

# # get zip files
# (zip_path <- 'input_data/nhgis0060_shape')
(zip_path <- '../biodiversity_in_a_concrete_jungle_data_too_big/wetlands/zipped_wetlands')
(zip_files<- list.files(zip_path, full.names = TRUE))
(out_path <- '../biodiversity_in_a_concrete_jungle_data_too_big/wetlands/')

# done once, no need to re-do
tic(); for(i in zip_files){
  print(i)
  unzip(i, exdir = out_path)
  file.remove(i)
}; toc() # ~3 mins

# remove the zips? Yes.
file.remove('../biodiversity_in_a_concrete_jungle_data_too_big/wetlands/zipped_wetlands')

# remove unnecesary files..
# Wetlands AND NOT HISTORIC OR RIPARIAN
# 
# Yes
# Wetlands
# Riparian
# 
# No
# Project
# Metadata
# Historic

# # tibble of all of the contents
# (
#   shape_names <- 
#     tibble(filename = list.files('../biodiversity_in_a_concrete_jungle_data_too_big/wetlands/'
#                                  , recursive = TRUE
#                                  ))
#   )
# 
# # list of things to keep
# (
#   files_to_keep <- # will reduce to just *.shp in a second
#     shape_names |> #View()
#     filter(str_detect(filename, paste(c("Wetlands", "Riparian"), collapse = '|'))) |> # has Either wetlands or riparian
#     filter(str_detect(filename, "Historic", negate = TRUE)) |> # drop historic
#     filter(str_detect(filename, "Project", negate = TRUE)) |> # view() # drop project
#     filter(str_detect(filename, "Metadata", negate = TRUE)) |> # view() # drop project
#     pull(filename)
#   )
# 
# length(files_to_keep) / 8 #:-/
# 
# files_to_remove <- 
#   shape_names |> 
#   filter(filename %nin% files_to_keep)
# 
# files_to_remove |> View() # looks good.
# 
# for(i in files_to_remove){
#   print(i)
#   file.remove(paste0('../biodiversity_in_a_concrete_jungle_data_too_big/wetlands/', i))
# }
# 
# rm(files_to_keep, files_to_remove) # clean up

```


# 3 examine
## A gdb - defunct
```{r eval=FALSE, include=FALSE}
list.files('../biodiversity_in_a_concrete_jungle_data_too_big/wetlands/')
st_layers('../biodiversity_in_a_concrete_jungle_data_too_big/wetlands/WV_geodatabase_wetlands.gdb')

slug <- '../biodiversity_in_a_concrete_jungle_data_too_big/wetlands/WV_geodatabase_wetlands.gdb'
wv_nwi <- 
  st_read(
    slug
    # , 'WV_Wetlands_Project_Metadata' # grid with meta
    # , 'WV_Wetlands_Historic_Map_Info'  # less-extensive grid, with meta
    # , 'West_Virginia' # state outline w grids.
    , 'WV_Wetlands'
  )

# 2x checks
wv_nwi |> glimpse()
wv_nwi |> 
  st_drop_geometry() |> 
  tabyl(WETLAND_TYPE)

wv_nwi |> 
  sample_n(1000) |> 
  mapview()

```

## B shp
```{r}


shapes_to_iterate_over <- 
  tibble(filenames = 
           list.files('../biodiversity_in_a_concrete_jungle_data_too_big/wetlands/'
                      , recursive = TRUE
                      , pattern = 'shp$'
                      , full.names = TRUE
                      )
         ) |> 
  mutate(state = str_remove_all(filenames, '../biodiversity_in_a_concrete_jungle_data_too_big/wetlands//') |>
           str_sub(1,2)
         , item = str_remove_all(filenames, '../biodiversity_in_a_concrete_jungle_data_too_big/wetlands//') |>
           str_remove_all('shapefile_wetlands') |>
           str_remove_all('.shp') |>
           str_remove('.._\\/')
         )

shapes_to_iterate_over$item

random_shape <- 
  shapes_to_iterate_over |> 
  sample_n(1) |>
  mutate(shp = map(filenames, ~st_read(.))) |> 
  unnest(shp)


# 2x checks
random_shape |> glimpse()
random_shape |> 
  st_drop_geometry() |> 
  tabyl(WETLAND_TY)

random_shape |> 
  st_as_sf() |> 
  sample_n(1000) |> 
  mapview()

```


# test another state out.. ugh overlaps, large duplicative area - what to do? Clipping geom reduces to state-specific union of MSA, UA, and HOLC boundaries
```{r eval=FALSE, include=FALSE}
list.files('../biodiversity_in_a_concrete_jungle_data_too_big/wetlands/')
st_layers('../biodiversity_in_a_concrete_jungle_data_too_big/wetlands/MD_geodatabase_wetlands.gdb')

slug <- '../biodiversity_in_a_concrete_jungle_data_too_big/wetlands/MD_geodatabase_wetlands.gdb'
md_nwi <- 
  st_read(
    slug
    # , 'WV_Wetlands_Project_Metadata' # grid with meta
    # , 'WV_Wetlands_Historic_Map_Info'  # less-extensive grid, with meta
    # , 'Maryland' # state outline w grids.
    , 'WV_Wetlands'
  )

# 2x checks
md_nwi |> glimpse()
md_nwi |> 
  st_drop_geometry() |> 
  tabyl(WETLAND_TYPE)

md_nwi |> 
  sample_n(1000) |> 
  mapview()

```



# 4 clipping geometry - old outdated
```{r eval=FALSE, include=FALSE}

sf_use_s2(FALSE) # suppresses errors, allows st_erase to run


nwi_crs <- 
  st_read(
    '../biodiversity_in_a_concrete_jungle_data_too_big/wetlands/WV_geodatabase_wetlands.gdb'
    , 'WV_Wetlands'
  ) |> st_crs()


list.files('working_data/holc_polys_saved')

msa  <- st_read('working_data/MSA/msa_as_geopackage_2022-12-08 15-40-07.gpkg') |> select() |> summarise()
ua   <- st_read('working_data/UA/UA_as_geopackage_2023-01-23 15-48-08.gpkg') |> select() |> summarise()
holc <- st_read('working_data/holc_polys_saved/holc_plys_2022-12-08 20-13-17.gpkg') |> select() |> summarise()


clipper_geom_temp  <- st_union(msa, ua)
clipper_geom       <- st_union(clipper_geom_temp, holc) |> st_transform(crs = nwi_crs)

clipper_geom |> mapview()

# clean things up
rm(msa, ua, holc)
rm(clipper_geom_temp)



```


# 4 clipping geometry - by state
## A get state boundaries
```{r}

(state_boundaries <- 
  get_acs(geography = 'state'
          , variables = "B19013_001"
          , state = states$value
          , year = 2020
          , geometry = TRUE) |> 
  st_transform(crs = st_crs(msa)) |> 
  select(census_name = NAME))

```


## B consolidate geometries to clip NWI down to: MSA, UA, HOLC
```{r}

sf_use_s2(FALSE) # suppresses errors, allows overlays to run


nwi_crs <- random_shape |> st_as_sf() |> st_crs()


# list.files('working_data/holc_polys_saved')


msa_state  <- 
  st_read('working_data/MSA/msa_as_geopackage_2022-12-08 15-40-07.gpkg') |> 
  select(msa_name) |> 
  mutate(msa_name = str_remove(msa_name,  ' Metro Area') |> 
           str_remove(' Micro Area')) |> 
  separate(msa_name, into = c('msa_text', 'state'), sep = ', ') |> 
  separate(state, into = c('state1', 'state2', 'state3', 'state4'), sep = '-') |> 
  pivot_longer(cols = starts_with('state'), values_to = 'state') |> 
  drop_na(state) |> 
  group_by(state) |> 
  summarise() |> 
  st_intersection(state_boundaries) |> 
  group_by(census_name) |> 
  summarise()

# look
msa_state |> mapview()

# # do MSA's fully contain HOLC polygons
# something is wrong with the HOLC polygons!
# tic(); test_overlap <- 
#   st_parallel(
#       msa_state
#     , st_difference
#     , 6
#     , holc
#     ); toc()


# # repeat the breaking up
ua_state <-
  st_read('working_data/UA/UA_as_geopackage_2023-01-23 15-48-08.gpkg') |> 
  select(ua_name = NAME) |> 
  mutate(ua_name = str_remove(ua_name, ' Urbanized Area \\(2010\\)') |> 
           str_remove(' Urban Cluster \\(2010\\)') |> 
           str_replace_all('--', ',')) |> 
  separate(ua_name, into = c('ua_text', 'state'), sep = ', ') |> 
  separate(state, into = c('state1', 'state2', 'state3', 'state4')) |> 
  pivot_longer(cols = starts_with('state'), values_to = 'state') |> 
  drop_na(state) |> 
  group_by(state) |> 
  summarise() |> 
  st_intersection(state_boundaries) |> 
  group_by(census_name) |> 
  summarise()

msa_state |> mapview() + mapview(ua_state) # very nice


tic(); (
  clipper_geom <- 
    msa_state |> 
    st_cast('MULTIPOLYGON') |> # why?
    bind_rows(ua_state) |> 
    group_by(census_name) |> 
    summarise() |> 
    st_transform(crs = nwi_crs) |> 
    left_join(
      fips_codes |> distinct(state, state_name)
      , by = c('census_name' = 'state_name')
      ) |> 
    select(census_name, state) |> 
    # st_simplify() |> # NEW
    st_make_valid() |> # NEW
    filter(st_is(geom, c('POLYGON', 'MULTIPOLYGON'))) # NEW
  ); toc() # ~3 seconds

clipper_geom |> 
  # slice(1) |> 
  mapview()

clipper_geom

# clean things up
rm(msa_state, ua_state)
rm(msa)




```


# 5 bring in a state, clip its water down, save out (just cuz), read back in and summarize per holc poly
TODO add other (NON-HOLC) polygons
```{r}


rm(random_shape)


holc <-
  st_read('working_data/holc_polys_saved/holc_plys_2022-12-08 20-13-17.gpkg') |> 
  st_make_valid()

ua_ungraded <- 
  st_read('working_data/UA_donut/UA_ungraded_2023-01-23 16-05-42.gpkg') |> 
  select(ua_GEOID = GEOID) |> 
  st_transform(crs = st_crs(holc))

msa_ungraded <- 
  st_read('working_data/MSA_donut/msa_ungraded.gpkg') |> 
  select(msa_GEOID) |> 
  st_transform(crs = st_crs(holc))




parallelly::availableCores() # 16, yeah!
ncore <- 8  # How many do you want to use?
# ncore <- 4  # How many do you want to use?

set.seed(1); gc()

tic(); for(i in shapes_to_iterate_over$filenames){
  tic()
  print('==============================================================================================')
  print(i)
  
  # get state code
  st_code <-
    shapes_to_iterate_over |> 
    filter(filenames == i) |> 
    pull(state)
  
  # get 'item' for naming
  item <-
    shapes_to_iterate_over |> 
    filter(filenames == i) |> 
    pull(item)
  
  # read in wetlands/riparian area
  tic(); nwi <- 
    st_read(i
            , query = paste0('SELECT WETLAND_TY as wetland_type FROM ', item) # needed?
            ); toc(); beep()
  
  # reduce clipper to state
  (clipper_geom_temp <- clipper_geom |> filter(state == st_code))
  
  # intersect item per state
  tic(); clipped_nwi <- 
    st_parallel(
        nwi
      , st_intersection
      , ncore
      , clipper_geom_temp # pre-selection, saves SOOO much time.
      ) |> 
    select(wetland_type, state) |> 
    filter(st_is(geometry, c('POLYGON', 'MULTIPOLYGON'))) |> # drop linestrings, if any
    st_cast('POLYGON') 
  
  # free up some memory
  rm(nwi)
  
  # write out clipped parquet file of
  clipped_nwi |> 
    st_write_parquet(paste0('../biodiversity_in_a_concrete_jungle_data_too_big/wetlands_clipped/', item, '_nwi.parquet')); toc(); beep() 
  
  # clipped nwi reproject to match holc polys
  tic(); clipped_nwi <- 
    clipped_nwi |> 
    st_transform(crs = st_crs(holc)); toc(); beep()
  
  # read in reduced holc polygons
  holc_temp <- 
    holc |>
    select(id, state) |> 
    filter(state == st_code) |> 
    select(id)
  
  # intersect clipped nwi with holc polygons, calculate overlap per poly AND per type of wetland
  tic(); st_parallel(
      clipped_nwi
    , st_intersection
    , ncore
    , holc_temp
    ) %>%
    mutate(area_nwi_km2 = as.double(st_area(.) / 1000000)) |> 
    st_drop_geometry() |> 
    group_by(id, wetland_type) |> # holc polygon AND by wetland type
    summarise(sum_area_nwi_km2 = sum(area_nwi_km2, na.rm = TRUE)) |> 
    write_csv(paste0('../biodiversity_in_a_concrete_jungle_data_too_big/wetlands_int/holc_', item, '_nwi.csv')); toc(); beep()
  
  # URBAN AREA polygons, calculate overlap per poly AND per type of wetland
  tic(); st_parallel(
      clipped_nwi
    , st_intersection
    , ncore
    , ua_ungraded
    ) %>%
    mutate(area_nwi_km2 = as.double(st_area(.) / 1000000)) |> 
    st_drop_geometry() |> 
    group_by(ua_GEOID, wetland_type) |> # msa polygon AND by wetland type
    summarise(sum_area_nwi_km2 = sum(area_nwi_km2, na.rm = TRUE)) |> 
    write_csv(paste0('../biodiversity_in_a_concrete_jungle_data_too_big/wetlands_int/ua_', item, '_nwi.csv')); toc(); beep()
  
  # MSA polygons, calculate overlap per poly AND per type of wetland
  tic(); st_parallel(
      clipped_nwi
    , st_intersection
    , ncore
    , msa_ungraded
    ) %>%
    mutate(area_nwi_km2 = as.double(st_area(.) / 1000000)) |> 
    st_drop_geometry() |> 
    group_by(msa_GEOID, wetland_type) |> # msa polygon AND by wetland type
    summarise(sum_area_nwi_km2 = sum(area_nwi_km2, na.rm = TRUE)) |> 
    write_csv(paste0('../biodiversity_in_a_concrete_jungle_data_too_big/wetlands_int/msa_', item, '_nwi.csv')); toc(); beep()
  
  toc(); beep()
  gc()
}; toc()



```

old and ewwwey

```{r}
# tic(); for(i in states[c(1, 2),]){ # for testing
# tic(); for(i in states$value){
# tic(); for(i in states$value[c(32: 40)]){ # MI, MS, NY failed
# tic(); for(i in states$value){
# tic(); for(i in states$value[c((16:40))]){ # MI failed
# tic(); for(i in states$value[c((40:19))]){ # SC failed
  tic()
  print('==============================================================================================')
  print(i)
  
  # TODO update
  path <- paste0('../biodiversity_in_a_concrete_jungle_data_too_big/wetlands/', i, '_geodatabase_wetlands.gdb')
  
  # # OG    
  # query = 'SELECT * FROM \"combined_blocks_2020_no_water_single_part\" WHERE GISJOIN = \"G06003702060102000\"'
  
  # tic(); nwi <- 
  #   st_read(path, paste0(i, '_Wetlands')) |> 
  #   select(attribute = ATTRIBUTE, wetland_type = WETLAND_TYPE) |> 
  #   rowid_to_column(); toc()
  # 
  tic(); nwi <- 
    st_read(path
            # , query = paste0('SELECT attribute = ATTRIBUTE, WETLAND_TYPE FROM ', i, '_Wetlands limit 10') # for testing
            , query = paste0('SELECT ATTRIBUTE as attribute, WETLAND_TYPE as wetland_type FROM ', i, '_Wetlands')
            ) |> 
    # st_as_sf() |> # change type
    rowid_to_column(); toc(); beepr::beep() # ~2 mins for SC
  
  tic(); check <- nwi |> st_is_valid(reason = TRUE); toc(); beepr::beep() # ~11 mins for SC
  
  # tic(); nwi <-
  #   st_read(path
  #           , promote_to_multi = FALSE
  #           , as_tibble = TRUE
  #           , query = paste0('SELECT attribute = ATTRIBUTE, WETLAND_TYPE FROM ', i, '_Wetlands limit 11000') # for testing
  #           # , query = paste0('SELECT ATTRIBUTE as attribute, WETLAND_TYPE as wetland_type FROM ', i, '_Wetlands')
  #           ) |>
  #   # st_make_valid(); toc(); beepr::beep()
  #   mutate(Shape = st_as_sfc(geos::geos_make_valid(Shape))); toc(); beepr::beep() # ~2 mins for SC
  #   # mutate(Shape = st_as_sfc(Shape)
#            
# test <- nwi |> sample_n(1000) |> st_cast('MULTIPOLYGON') |> mutate(Shape = st_as_sfc(geos::geos_make_valid(Shape)))
  
  # table(check)
  
  invalids <- 
    check |> 
    tibble() |> 
    rowid_to_column() |> 
    filter(check != 'Valid Geometry')
  
  # pull out the broken ones and fix
  tic(); nwi_fix <-
    nwi |> 
    filter(rowid %in% invalids$rowid) |> 
    st_make_valid() |> 
    st_buffer(0); toc()#|> mapview()
  
  # set working ones aside
  tic(); nwi_valid <- 
    nwi |> 
    anti_join(invalids, by = 'rowid'); toc() # ~12 mins
  
  # # drop the invalids, bind the fixed invalids back on
  # tic(); nwi <-
  #   nwi |> 
  #   anti_join(invalids, by = 'rowid') |> 
  #   # filter(rowid %nin% invalids$rowid) |> # NOT IN
  #   bind_rows(nwi_fix); toc()

  beepr::beep()
  
  # reduce clipper
  clipper_geom_temp <- clipper_geom |> filter(state == i)
  
  # intersect working ones
  tic(); nwi_int <- 
    st_parallel(
      nwi
        # nwi_valid |> st_make_valid()
      , st_intersection
      , ncore
      , clipper_geom_temp # pre-selection, saves SOOO much time.
      ) |> 
    # mutate(area_nwi_km2 = as.double(st_area(.) / 1000000)) |> 
    select(wetland_type, state) |> 
    filter(st_is(geometry, c('POLYGON', 'MULTIPOLYGON'))); toc(); beepr::beep() # drop linestrings, if any
  
  # intersect previously broken ones
    tic(); nwi_int_fix <- 
    st_parallel(
      # nwi
        nwi_fix
      , st_intersection
      , ncore
      , clipper_geom_temp # pre-selection, saves SOOO much time.
      ) |> 
    # mutate(area_nwi_km2 = as.double(st_area(.) / 1000000)) |> 
    select(wetland_type, state) |> 
    filter(st_is(Shape, c('POLYGON', 'MULTIPOLYGON'))) |> 
      st_cast('POLLYGON'); toc(); beepr::beep() # drop linestrings, if any
    
    # combine
    nwi_int |> 
      bind_rows(nwi_int_fix) |> 
      st_write_parquet(paste0('../biodiversity_in_a_concrete_jungle_data_too_big/wetlands_clipped/', i, '_nwi.parquet')); toc()
    # st_write(paste0('../biodiversity_in_a_concrete_jungle_data_too_big/wetlands_clipped/', i, '_nwi.gpkg')); toc()
  
  beepr::beep()
  
  clipped_nwi <- 
    st_read_parquet(
      paste0('../biodiversity_in_a_concrete_jungle_data_too_big/wetlands_clipped/', i, '_nwi.parquet')
    # st_read(
    #   paste0('../biodiversity_in_a_concrete_jungle_data_too_big/wetlands_clipped/', i, '_nwi.gpkg')
    ) |> 
    st_transform(crs = st_crs(holc))
  
  beepr::beep()

  holc_temp <- 
    holc |> 
    # st_read('working_data/holc_polys_saved/holc_plys_2022-12-08 20-13-17.gpkg') |> 
    select(id, state) |> 
    filter(state == i) |> 
    select(id)
  
  beepr::beep()
  
  st_parallel(
      clipped_nwi
    , st_intersection
    , ncore
    , holc_temp
    ) %>%
    mutate(area_nwi_km2 = as.double(st_area(.) / 1000000)) |> 
    st_drop_geometry() |> 
    group_by(id) |> 
    summarise(sum_area_nwi_km2 = sum(area_nwi_km2, na.rm = TRUE)) |> 
    write_csv(paste0('../biodiversity_in_a_concrete_jungle_data_too_big/wetlands_holc_int/', i, '_nwi.csv'))

  toc()
  beepr::beep()
  gc()
  }; toc(); beepr::beep()


# nwi_cliped
# nwi_cliped |> glimpse()
# nwi_cliped |> sample_n(1000) |> mapview()



```







